% Chapter 4
\chapter{Performance Analysis and Conclusions} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter4} 

%----------------------------------------------------------------------------------------

\section{Intelligibility Analysis}
On comparing the best performing training data for the three deep learning models as discussed in the chapter \ref{Chapter3}, the gain in the intelligibility can be tabulated as below:\\
\begin{table}[!htbp]
\centering
\begin{tabular}{|p{8cm}|p{4cm}|p{4cm}|}
\hline
\textbf{Deep Learning Model} & \textbf{STOI Gain at 0 SNR} & \textbf{STOI Gain at -2 SNR}\\
\hline
Baseline DNN & 12.3\% & 14.9\%\\
\hline
DNN with biased sigmoid activation & 9.6\% & 14.9\%\\
\hline
CNN with leaky reLU & 17.8\% & 19.4\%\\
\hline
\end{tabular}
\caption{Intelligibility Gains}
\label{tab:i_gain_compare}
\end{table}
\\
Hence, from \ref{tab:i_gain_compare} and observing the intelligibility scores of original noisy mixtures from the table \ref{tab:stoi_mix}, we can make following analysis:\\
\begin{itemize}
\item STOI for original noisy mixture at 0 SNR is 0.73. This is already a decent score (on the STOI metric scale of 0-1) and hence, the subsequent gain in intelligibility at 0 SNR post speech enhancement is lower than the STOI gain at -2 SNR for which the original noisy mixture has a STOI score of 0.67.
\item There is a clear trend of improvement of STOI gain from the noisy audio mixtures at 0 SNR to the noisy audio mixtures at -2 SNR. This affirms the ability of IRM in preserving the clean speech energies which is helpful for speech enhancement particularly in low SNR scenarios.
\item Second DNN with the biased sigmoid activation provides less gain in intelligibility scores for 0 SNR and the same score of intelligibility for the -2 SNR when compared to the baseline DNN. It's intelligibility gain when compare to the CNN based model is however poor. Using baseline DNN as a reference, for the dataset considered, the prima facie trend show slightly poor performance of the biased sigmoid layer comapred to the reLU activation used in the baseline. But, to make a conclusive remark training on a larger dataset would be required. 
\item CNN with leaky reLU shows the best intelligibility gains among the three considered deep learning models and also places the least demand on the feature extraction as only spectrogram/cochleagram were used in the training phase.
\item For DNN based learning models and the spectrogram based training target, the better performing feature set was always found to include the gammatone cepstral features i.e. the set \enquote{\textit{GFCC, GFCC delta, GFCC delta delta}}. However, for the training target based on cochleagram, the best performing feature set was always found to include the feature set  \enquote{\textit{Cochleagram}}.
\end{itemize}
%---------------------------------------------------------------------------
\section{Quality Analysis}
On comparing the best performing training data for the three deep learning models as discussed in the previous chapter \ref{Chapter3}, the gain in the quality scores considering MOSLQO has been tabulated in the table \ref{tab:q_gain_compare}. MOSLQO is chosen over PESQMOS as it defines perceptual quality gain with respect to the human listening objectivity.\\
\begin{table}[!htbp]
\centering
\begin{tabular}{|p{8cm}|p{3cm}|p{4cm}|}
\hline
\textbf{Deep Learning Model} & \textbf{Quality Gain at 0 SNR} & \textbf{Quality Gain at -2 SNR}\\
\hline
Baseline DNN & 13.1\% & 15.6\%\\
\hline
DNN with biased sigmoid activation  & 12.9\% & 13.8\%\\
\hline
CNN with leaky reLU & 20.7\% & 22.4\%\\
\hline
\end{tabular}
\caption{Quality Gains}
\label{tab:q_gain_compare}
\end{table}

Hence from the tables \ref{tab:q_gain_compare} and \ref{tab:pesq_mix}, the following analysis can be made for quality as a performance metric:
\begin{itemize}
\item MOSLQO scores for original noisy mixture at 0 SNR is 1.83. This is qualitatively at the lower scale of 1-5 for MOSLQO. The subsequent gain in quality metric at 0 SNR shows an improvement of 15.6\% on average. Likewise, quality scores at -2 SNR for original noisy mixtures is 1.73, but the quality gain is higher as compared to the 0 SNR case with an average gain of 17.2\%. This points out, and as also observed from all three learning models, the speech enhancement has more scope of audio quality improvement at the lower SNR levels when using IRM as the training target. 
\item DNN with biased sigmoid activation provides the least gain in the quality among the three deep learning models considered and hence, strengthen the argument that the reLU activation considered in the baseline DNN performs better than the biased sigmoid activation. However, to conclusively make this statement and as per the analysis from the intelligibility gains, training of both models on a larger dataset would be required.
\item CNN with leaky reLU shows the best quality gains among the three considered deep learning models along with the intelligibility as previously observed and also places the least demand on the feature extraction. This reaffirms the ability of this category of deep learning machines in better speech enhancement as compared to the other models.
\end{itemize}
%---------------------------------------------------------------------------
\section{\textbf{Conclusions}}
Based on study of the experiments conducted the following conclusions can be made:
\begin{itemize}
\item DNN based learning models perform better with cochleagram as training data and IRM based on cochleagram as the training target when compared with audio features and IRM based on spectrogram.
\item Using the feature set \enquote{GFCC, GFCC delta, GFCC delta delta} has the best performance for 0 SNR with DNN based learning models and IRM based on spectrogram as the training target.
\item Using the feature set \enquote{GFCC, GFCC delta, GFCC delta delta, Spectrogram} has the best performance for -2 SNR with DNN based learning models and IRM based on spectrogram as the training target. 
\item This highlights the importance of gammatone based cepstral features in speech enhancement where the learning machine places a significant demand on feature extraction as a part of training data.
\item CNN with leaky reLU activation has the best performance in terms of gains in intelligibility and quality scores out of all considered models. \item Performance of CNN also underlines the importance and power of this category of learning machines which have the ability to learn the patterns in the training data by themselves thus placing minimum demand on feature extraction. 
\item It's also noteworthy that the improvement in gain of both intelligibility and quality post speech enhancement is more in -2 SNR as compared to 0 SNR. Hence, for noisy mixtures with even lower SNR values, the scope of intelligibility and quality gain is established as per the observed trends from the experiments.
\end{itemize}
%---------------------------------------------------------------------------
\section{\textbf{Future Prospects}}
\begin{itemize}
\item \textbf{Train on larger dataset}:\\
All of the considered models were trained on the TIMIT dataset of 1718 audio samples. Hence, by expanding the dataset all of the models can be trained to generalize and learn \enquote{noise} from the noisy mixtures more efficiently thus helping in better speech enhancement and source separation. This can thus help in improvement of intelligibility and quality gains.\\ This would also help in conclusively answering the argument regarding the performance of the biased sigmoid layer with respect to the reLU layer.
\item \textbf{Train on lower SNR values}:\\
All of the considered models were trained for noisy mixtures at 0 and -2 SNR respectively. As per the analysis of performanc metrics, the gain in quality and intelligibility was more for noisy mixtures at -2 SNR post enhancement. By testing on noisy mixtures at even lower SNRs, this trend can be verified.
\item \textbf{Scale to binaural models}:\\ Using the Interaural Time Difference (ITD) and Interaural Intensity Difference(IID) as additional features, these models can be trained for binaural mixtures.
\item \textbf{Testing on more models}:\\ This involves exploring unsupervised learning as standalone or in tandem with the supervised learning  to check for any improvement in performance metrics.
\item \textbf{Weight Initialization}:\\ In the models implemented, the weights are uninitialized before training. This allowed the deep learning models to self initialize the weights randomly. Using unsupervised pretraining first, weights can be appropriately initialised to ensure fast arrival to the optimisation point during training.
\item \textbf{Real-time application}:\\ For the models and feature sets considered, there is an average latency of around 0.25 s during prediction phase from the time of feature extraction to the enhanced speech's reconstruction. This is when the testing dataset is readily available. For a real-time application which would involve binarization of the audio recording for storage first to the signal reconstruction post speech enhancement, there is enough potential for optimising the models for reducing latency further.
\item \textbf{Transfer Learning}:\\ Comparison of the performance evaluation for the models considered in the project with the well known neural networks like GoogLeNet , Alexnet etc can also be studied.
\end{itemize}